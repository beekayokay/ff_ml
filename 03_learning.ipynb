{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "load_dotenv()\n",
    "ROOT_DIR = os.getenv('FILES_DIR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "TEAMS = 12\n",
    "ROSTER_SIZE = 16\n",
    "STARTERS_NO_DEF_K = 1 + 2 + 2 + 1 + 1\n",
    "VBD_BASELINE = TEAMS * STARTERS_NO_DEF_K + TEAMS\n",
    "BUDGET = 200\n",
    "TOTAL_MONEY = (BUDGET - ROSTER_SIZE) * TEAMS\n",
    "RANK_TYPE_COLUMNS = [\n",
    "    'rank', 'rank_pos_rank', 'best', 'worst',\n",
    "    'rank_avg', 'adp_pos_rank', 'adp_avg'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "db_file = f'{ROOT_DIR}/ff_ml.db'\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "proj_df_raw = pd.read_sql('SELECT * FROM projections', conn)\n",
    "rank_df_raw = pd.read_sql('SELECT * FROM ranks', conn)\n",
    "adp_df_raw = pd.read_sql('SELECT * FROM adps', conn)\n",
    "stat_df_raw = pd.read_sql('SELECT * FROM stats', conn)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out irrelevant rows of stats\n",
    "stat_df_raw = stat_df_raw[stat_df_raw['g_played'] >= 13]\n",
    "stat_df_raw = stat_df_raw[stat_df_raw['pts/g'] >= 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format df's\n",
    "PROJ_COL_DROP = ['year', 'plyr', 'team']\n",
    "RANK_COL_DROP = ['year', 'plyr', 'pos', 'team', 'rank', 'pos_rank', 'best', 'worst', 'avg', 'std_dev', 'ecr_adp']\n",
    "ADP_COL_DROP = ['year', 'plyr', 'pos', 'team', 'pos_rank', 'avg']\n",
    "\n",
    "proj_df = proj_df_raw.copy()\n",
    "rank_df = rank_df_raw.copy()\n",
    "adp_df = adp_df_raw.copy()\n",
    "stat_df = stat_df_raw[['year-plyr-pos', 'pts/g']].copy()\n",
    "\n",
    "proj_df.drop(columns=PROJ_COL_DROP, inplace=True)\n",
    "rank_df.drop(columns=RANK_COL_DROP, inplace=True)\n",
    "adp_df.drop(columns=ADP_COL_DROP, inplace=True)\n",
    "\n",
    "rank_df.rename(columns={'pos_rank': 'rank_pos_rank', 'avg': 'rank_avg'}, inplace=True)\n",
    "adp_df.rename(columns={'pos_rank': 'adp_pos_rank', 'avg': 'adp_avg'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge prediction-based dfs and fix dtypes\n",
    "pred_df = proj_df.merge(rank_df, on='year-plyr-pos-team', how='inner')\n",
    "pred_df = pred_df.merge(adp_df, on='year-plyr-pos-team', how='inner')\n",
    "pred_df.drop(columns=['year-plyr-pos-team', 'year-plyr-pos_x', 'year-plyr-pos_y'], inplace=True)\n",
    "for column in list(pred_df.columns):\n",
    "    try:\n",
    "        pred_df = pred_df.astype({column: 'float32'})\n",
    "    except ValueError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge to stat df to create 1 training df\n",
    "df_train = stat_df.merge(pred_df, on='year-plyr-pos', how='inner')\n",
    "df_train.set_index('year-plyr-pos', inplace=True)\n",
    "\n",
    "# for column in list(df_train.columns):\n",
    "#     if column in RANK_TYPE_COLUMNS:\n",
    "#         df_train[column] = (TEAMS * ROSTER_SIZE) - df_train[column]\n",
    "\n",
    "print(df_train.head(5))\n",
    "print(df_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training df to pos and X and y\n",
    "qb_X_train = df_train[df_train['pos'] == 'qb'].copy().drop(columns=['pos', 'pts/g'])\n",
    "rb_X_train = df_train[df_train['pos'] == 'rb'].copy().drop(columns=['pos', 'pts/g'])\n",
    "wr_X_train = df_train[df_train['pos'] == 'wr'].copy().drop(columns=['pos', 'pts/g'])\n",
    "te_X_train = df_train[df_train['pos'] == 'te'].copy().drop(columns=['pos', 'pts/g'])\n",
    "qb_y_train = df_train[df_train['pos'] == 'qb']['pts/g'].copy()\n",
    "rb_y_train = df_train[df_train['pos'] == 'rb']['pts/g'].copy()\n",
    "wr_y_train = df_train[df_train['pos'] == 'wr']['pts/g'].copy()\n",
    "te_y_train = df_train[df_train['pos'] == 'te']['pts/g'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same thing for test data\n",
    "\n",
    "# get data\n",
    "db_file = f'{ROOT_DIR}/ff_ml.db'\n",
    "conn = sqlite3.connect(db_file)\n",
    "\n",
    "proj_df_current = pd.read_sql('SELECT * FROM projections_current', conn)\n",
    "rank_df_current = pd.read_sql('SELECT * FROM ranks_current', conn)\n",
    "adp_df_current = pd.read_sql('SELECT * FROM adps_current', conn)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get references for later\n",
    "player_ref = rank_df_current.merge(adp_df_current, on='year-plyr-pos', how='inner')\n",
    "\n",
    "vbd_df = adp_df_current[adp_df_current['avg'] <= TEAMS * ROSTER_SIZE]\n",
    "vbd_qb_baseline = sum(vbd_df['pos'] == 'qb')\n",
    "vbd_rb_baseline = sum(vbd_df['pos'] == 'rb')\n",
    "vbd_wr_baseline = sum(vbd_df['pos'] == 'wr')\n",
    "vbd_te_baseline = sum(vbd_df['pos'] == 'te')\n",
    "\n",
    "vbd_qb_starter = TEAMS\n",
    "vbd_rb_starter = TEAMS * 2\n",
    "vbd_wr_starter = TEAMS * 2\n",
    "vbd_te_starter = TEAMS\n",
    "\n",
    "vbd_qb_top_reserve = vbd_qb_starter * 1.5\n",
    "vbd_rb_top_reserve = vbd_rb_starter * 1.5\n",
    "vbd_wr_top_reserve = vbd_wr_starter * 1.5\n",
    "vbd_te_top_reserve = vbd_te_starter * 1.5\n",
    "\n",
    "vbd_qb_elite = vbd_qb_starter * 0.5\n",
    "vbd_rb_elite = vbd_rb_starter * 0.5\n",
    "vbd_wr_elite = vbd_wr_starter * 0.5\n",
    "vbd_te_elite = vbd_te_starter * 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format df's\n",
    "proj_df_current.drop(columns=PROJ_COL_DROP, inplace=True)\n",
    "rank_df_current.drop(columns=RANK_COL_DROP, inplace=True)\n",
    "adp_df_current.drop(columns=ADP_COL_DROP, inplace=True)\n",
    "\n",
    "rank_df_current.rename(columns={'pos_rank': 'rank_pos_rank', 'avg': 'rank_avg'}, inplace=True)\n",
    "adp_df_current.rename(columns={'pos_rank': 'adp_pos_rank', 'avg': 'adp_avg'}, inplace=True)\n",
    "\n",
    "# merge prediction-based dfs and fix dtypes\n",
    "df_test = proj_df_current.merge(rank_df_current, on='year-plyr-pos-team', how='inner')\n",
    "df_test = df_test.merge(adp_df_current, on='year-plyr-pos-team', how='inner')\n",
    "df_test.drop(columns=['year-plyr-pos-team', 'year-plyr-pos_x', 'year-plyr-pos_y'], inplace=True)\n",
    "for column in list(df_test.columns):\n",
    "    try:\n",
    "        df_test = df_test.astype({column: 'float64'})\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "df_test.set_index('year-plyr-pos', inplace=True)\n",
    "\n",
    "for column in list(df_test.columns):\n",
    "    if column in RANK_TYPE_COLUMNS:\n",
    "        df_test[column] = (TEAMS * ROSTER_SIZE) - df_test[column]\n",
    "\n",
    "print(df_test.head(5))\n",
    "print(df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split test df to pos and X and y\n",
    "qb_X_test = df_test[df_test['pos'] == 'qb'].copy().drop(columns=['pos'])\n",
    "rb_X_test = df_test[df_test['pos'] == 'rb'].copy().drop(columns=['pos'])\n",
    "wr_X_test = df_test[df_test['pos'] == 'wr'].copy().drop(columns=['pos'])\n",
    "te_X_test = df_test[df_test['pos'] == 'te'].copy().drop(columns=['pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction model\n",
    "# model determined by model_analysis\n",
    "\n",
    "def regressor(X_train, y_train, X_test, model, degree, reference):\n",
    "    poly_features = PolynomialFeatures(degree=degree, interaction_only=True)\n",
    "    sc = StandardScaler()\n",
    "    minmaxsc = MinMaxScaler(feature_range=(0,1))\n",
    "    pca = PCA()\n",
    "    poly_reg = Pipeline([\n",
    "        ('poly', poly_features),\n",
    "        ('minmax', minmaxsc),\n",
    "        ('pca', pca),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "    poly_reg.fit(X_train, y_train)\n",
    "    pred_y = poly_reg.predict(X_test)\n",
    "    df = pd.DataFrame({'name':X_test.index, 'pts':pred_y})\n",
    "    # df.sort_values(by=['pts'], ascending=False, inplace=True)\n",
    "    # df.reset_index(drop=True, inplace=True)\n",
    "    return df\n",
    "\n",
    "qb_models = {\n",
    "    linear_model.ElasticNet(alpha=0.25): 2,\n",
    "}\n",
    "rb_models = {\n",
    "    linear_model.ElasticNet(alpha=0.25): 2\n",
    "}\n",
    "wr_models = {\n",
    "    linear_model.ElasticNet(alpha=0.25): 2\n",
    "}\n",
    "te_models = {\n",
    "    linear_model.ElasticNet(alpha=0.25): 2\n",
    "}\n",
    "\n",
    "for idx, qb_model in enumerate(qb_models):\n",
    "    temp_df = regressor(qb_X_train, qb_y_train, qb_X_test, qb_model, qb_models[qb_model], 'qb')\n",
    "    if idx == 0:\n",
    "        qb_pred_df = temp_df\n",
    "    else:\n",
    "        qb_pred_df = pd.concat((qb_pred_df, temp_df))\n",
    "\n",
    "for idx, rb_model in enumerate(rb_models):\n",
    "    temp_df = regressor(rb_X_train, rb_y_train, rb_X_test, rb_model, rb_models[rb_model], 'rb')\n",
    "    if idx == 0:\n",
    "        rb_pred_df = temp_df\n",
    "    else:\n",
    "        rb_pred_df = pd.concat((rb_pred_df, temp_df))\n",
    "\n",
    "for idx, wr_model in enumerate(wr_models):\n",
    "    temp_df = regressor(wr_X_train, wr_y_train, wr_X_test, wr_model, wr_models[wr_model], 'wr')\n",
    "    if idx == 0:\n",
    "        wr_pred_df = temp_df\n",
    "    else:\n",
    "        wr_pred_df = pd.concat((wr_pred_df, temp_df))\n",
    "\n",
    "for idx, te_model in enumerate(te_models):\n",
    "    temp_df = regressor(te_X_train, te_y_train, te_X_test, te_model, te_models[te_model], 'te')\n",
    "    if idx == 0:\n",
    "        te_pred_df = temp_df\n",
    "    else:\n",
    "        te_pred_df = pd.concat((te_pred_df, temp_df))\n",
    "\n",
    "qb_pred_df = qb_pred_df.groupby('name', as_index=False).mean()\n",
    "qb_pred_df.sort_values(by=['pts'], ascending=False, inplace=True)\n",
    "qb_pred_df.reset_index(drop=True, inplace=True)\n",
    "rb_pred_df = rb_pred_df.groupby('name', as_index=False).mean()\n",
    "rb_pred_df.sort_values(by=['pts'], ascending=False, inplace=True)\n",
    "rb_pred_df.reset_index(drop=True, inplace=True)\n",
    "wr_pred_df = wr_pred_df.groupby('name', as_index=False).mean()\n",
    "wr_pred_df.sort_values(by=['pts'], ascending=False, inplace=True)\n",
    "wr_pred_df.reset_index(drop=True, inplace=True)\n",
    "te_pred_df = te_pred_df.groupby('name', as_index=False).mean()\n",
    "te_pred_df.sort_values(by=['pts'], ascending=False, inplace=True)\n",
    "te_pred_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set vbd\n",
    "qb_baseline = qb_pred_df.loc[vbd_qb_baseline-1, 'pts']\n",
    "rb_baseline = rb_pred_df.loc[vbd_rb_baseline-1, 'pts']\n",
    "wr_baseline = wr_pred_df.loc[vbd_wr_baseline-1, 'pts']\n",
    "te_baseline = te_pred_df.loc[vbd_te_baseline-1, 'pts']\n",
    "\n",
    "qb_top_reserve = qb_pred_df.loc[vbd_qb_top_reserve-1, 'pts']\n",
    "rb_top_reserve = rb_pred_df.loc[vbd_rb_top_reserve-1, 'pts']\n",
    "wr_top_reserve = wr_pred_df.loc[vbd_wr_top_reserve-1, 'pts']\n",
    "te_top_reserve = te_pred_df.loc[vbd_te_top_reserve-1, 'pts']\n",
    "\n",
    "qb_starter = qb_pred_df.loc[vbd_qb_starter-1, 'pts']\n",
    "rb_starter = rb_pred_df.loc[vbd_rb_starter-1, 'pts']\n",
    "wr_starter = wr_pred_df.loc[vbd_wr_starter-1, 'pts']\n",
    "te_starter = te_pred_df.loc[vbd_te_starter-1, 'pts']\n",
    "\n",
    "qb_elite = qb_pred_df.loc[vbd_qb_elite-1, 'pts']\n",
    "rb_elite = rb_pred_df.loc[vbd_rb_elite-1, 'pts']\n",
    "wr_elite = wr_pred_df.loc[vbd_wr_elite-1, 'pts']\n",
    "te_elite = te_pred_df.loc[vbd_te_elite-1, 'pts']\n",
    "\n",
    "qb_pred_df['vbd_baseline'] = qb_pred_df['pts'] - qb_baseline\n",
    "rb_pred_df['vbd_baseline'] = rb_pred_df['pts'] - rb_baseline\n",
    "wr_pred_df['vbd_baseline'] = wr_pred_df['pts'] - wr_baseline\n",
    "te_pred_df['vbd_baseline'] = te_pred_df['pts'] - te_baseline\n",
    "\n",
    "qb_pred_df['vbd_top_reserve'] = qb_pred_df['pts'] - qb_top_reserve\n",
    "rb_pred_df['vbd_top_reserve'] = rb_pred_df['pts'] - rb_top_reserve\n",
    "wr_pred_df['vbd_top_reserve'] = wr_pred_df['pts'] - wr_top_reserve\n",
    "te_pred_df['vbd_top_reserve'] = te_pred_df['pts'] - te_top_reserve\n",
    "\n",
    "qb_pred_df['vbd_starter'] = qb_pred_df['pts'] - qb_starter\n",
    "rb_pred_df['vbd_starter'] = rb_pred_df['pts'] - rb_starter\n",
    "wr_pred_df['vbd_starter'] = wr_pred_df['pts'] - wr_starter\n",
    "te_pred_df['vbd_starter'] = te_pred_df['pts'] - te_starter\n",
    "\n",
    "qb_pred_df['vbd_elite'] = qb_pred_df['pts'] - qb_elite\n",
    "rb_pred_df['vbd_elite'] = rb_pred_df['pts'] - rb_elite\n",
    "wr_pred_df['vbd_elite'] = wr_pred_df['pts'] - wr_elite\n",
    "te_pred_df['vbd_elite'] = te_pred_df['pts'] - te_elite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.concat([qb_pred_df, rb_pred_df, wr_pred_df, te_pred_df])\n",
    "pred_df = pred_df.merge(player_ref[['year-plyr-pos', 'plyr_x', 'pos_x', 'team_x', 'avg_x', 'avg_y']], left_on='name', right_on='year-plyr-pos')\n",
    "pred_df.drop(columns=['name', 'year-plyr-pos'], inplace=True)\n",
    "pred_df.rename(columns={\n",
    "    'plyr_x': 'plyr',\n",
    "    'pos_x': 'pos',\n",
    "    'team_x': 'team', \n",
    "    'avg_x': 'rank',\n",
    "    'avg_y': 'adp'\n",
    "}, inplace=True)\n",
    "pred_df = pred_df[['plyr', 'pos', 'team', 'rank', 'adp', 'pts', 'vbd_baseline', 'vbd_top_reserve', 'vbd_starter', 'vbd_elite']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df['vbd_sum'] = 0\n",
    "for index, row in pred_df.iterrows():\n",
    "    temp_vbd_sum = 0\n",
    "    if pred_df.loc[index, 'vbd_baseline'] < 0:\n",
    "        temp_vbd_sum += 0\n",
    "    else:\n",
    "        temp_vbd_sum += pred_df.loc[index, 'vbd_baseline']\n",
    "    if pred_df.loc[index, 'vbd_top_reserve'] < 0:\n",
    "        temp_vbd_sum += 0\n",
    "    else:\n",
    "        temp_vbd_sum += pred_df.loc[index, 'vbd_top_reserve']\n",
    "    if pred_df.loc[index, 'vbd_starter'] < 0:\n",
    "        temp_vbd_sum += 0\n",
    "    else:\n",
    "        temp_vbd_sum += pred_df.loc[index, 'vbd_starter']\n",
    "    if pred_df.loc[index, 'vbd_elite'] < 0:\n",
    "        temp_vbd_sum += 0\n",
    "    else:\n",
    "        temp_vbd_sum += pred_df.loc[index, 'vbd_elite']\n",
    "    pred_df.loc[index, 'vbd_sum'] = temp_vbd_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_pos_vbd = pred_df[pred_df['vbd_sum'] >= 0]['vbd_sum'].sum()\n",
    "cost_per_vbd = TOTAL_MONEY / total_pos_vbd\n",
    "pred_df['auction_value'] = pred_df['vbd_sum'] * cost_per_vbd\n",
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_df.to_csv(f'{ROOT_DIR}/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regressor(X_train, y_train, X_test, model, degree):\n",
    "    poly_features = PolynomialFeatures(degree=degree, interaction_only=True)\n",
    "    sc = StandardScaler()\n",
    "    minmaxsc = MinMaxScaler(feature_range=(0,1))\n",
    "    pca = PCA()\n",
    "    poly_reg = Pipeline([\n",
    "        ('poly', poly_features),\n",
    "        ('minmax', minmaxsc),\n",
    "        ('pca', pca),\n",
    "        ('regressor', model)\n",
    "    ])\n",
    "    poly_reg.fit(X_train, y_train)\n",
    "    fitted_regressor = poly_reg.named_steps['regressor']\n",
    "    coefficients = fitted_regressor.coef_\n",
    "    poly_feature_names = poly_reg.named_steps['poly'].get_feature_names_out()\n",
    "    pca_feature_names = poly_feature_names[:poly_reg.named_steps['pca'].n_components_]\n",
    "\n",
    "    df = pd.DataFrame([coefficients[:len(pca_feature_names)]], columns=pca_feature_names).T\n",
    "    return df\n",
    "\n",
    "models = {\n",
    "    linear_model.ElasticNet(alpha=0.25): 2,\n",
    "}\n",
    "\n",
    "analysis_X_train = pd.concat([qb_X_train, rb_X_train, wr_X_train, te_X_train])\n",
    "analysis_y_train = pd.concat([qb_y_train, rb_y_train, wr_y_train, te_y_train])\n",
    "analysis_X_test = pd.concat([qb_X_test, rb_X_test, wr_X_test, te_X_test])\n",
    "\n",
    "for idx, model in enumerate(models):\n",
    "    temp_df = regressor(analysis_X_train, analysis_y_train, analysis_X_test, model, models[model])\n",
    "\n",
    "# temp_df.to_csv(f'{ROOT_DIR}/analysis_of_current_model.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_X_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ff_ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
